1. I first start with HDFS commands in the hdfs_commands.txt file, doing all the commands under making the necessary directories. I follow the directory structure as specified in the instructions (one for the analytic, one for data ingestion, one for cleaning, one for profiling, one for testing/unused code).

2. Then, I move onto the data profiling steps. After uploading the raw data file (NFL.csv) into Dataproc, I run code_profiling_1.scala (which is also uploaded into Dataproc). This gives profiles the numerical columns of the data by giving us the mean, median, and mode for all numerical columns and the standard deviation of the 40-yard time column.

3. I then go onto profiling the non-numerical data. First, I run code_profiling_2.scala (after uploading to Dataproc) which prepares the data for profiling by first getting just the Team name of each player by parsing through the "Drafted..tm.rnd.yr." column and keeping only the non-numerical columns of the data (excluding player name). I make sure to download the modified data from HDFS using the corresponding command in hdfs_commands.txt. After downloading this data onto my local machine and renaming it NFlProfilingNonNumeric.csv, I reupload it to Dataproc and put it into the data_ingest directory in HDFS. Then, after going into Hive, I run all the queries in code_profiling_non_numeric.hql. This file creates a table that ingests the data in the NFlProfilingNonNumeric.csv then goes on to display the unique values in each column.

4. Then, I go on to clean the code. I do this by running the code_cleaning_final.scala file (after uploading to Dataproc). This file cleans up the player name column (by keeping just their actual names), gets just the round value for each player by parsing through the "Drafted..tm.rnd.yr." column (also cleaning this up by putting "Undrafted" for undrafted players), and keeps only the necessary columns in the data and drops the rest (only keeping player name, position, 40-yard time, and round). I also include rows where the Position column is QB, WR, TE, or RB. I make sure to download the modified data from HDFS using the corresponding command in hdfs_commands.txt. After downloading this data onto my local machine and renaming it NFLClean.csv, I reupload it to Dataproc and put it into the data_ingest directory in HDFS. I put my partner's cleanedCombineData.csv also in the data_ingest directory in HDFS.

5. Now, I do the data analysis. After going into Hive, I run all the queries in code_analytics.hql. This file creates a table that ingests the data in the NFLClean.csv (my dataset), creates another table that ingests the data in the cleanedCombineData.csv dataset (my partner's dataset), combines the two into a new combined dataset (and exports it to HDFS), creates another table to store our analytics, and then fills that table with our desired analytics (which is the number of players, mean 40-yard time, standard deviation 40-yard time, minimum 40-yard time, maximum 40-yard time, and number of missing 40-yard times for each type of position within each round). The results are then displayed and exported to HDFS (ana_code directory). The combined data is also exported to HDFS (etl_code directory).

6. Finally, I put everything into the corresponding folders in HDFS (last commands in the hdfs_commands.txt file).

Notes:
* Screenshots of runs are in the attached PDF